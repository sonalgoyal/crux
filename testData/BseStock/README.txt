Steps to populate demo data in HBase

1.Create tables in HBase

hbase(main):006:0>  create 'stockDataComposite','price','spread','stats'
hbase(main):007:0>  create 'stockDataSimple','data'   

2.Run hadoop jobs to export data to HBase tables. You will need to set HADOOP_HOME and HBASE_HOME
a. Add all jars in $HBASE_HOME/lib to HADOOP_CLASSPATH. To do so, add following line to $HADOOP_HOME/bin/hadoop file before the command
 <exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@" >

HBASE_HOME=<path to HBase folder>
# add Hbase libs to CLASSPATH
for f in $HBASE_HOME/lib/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done 

Note: These lines will add hbase dependencies to your hadoop classpath each time you run hadoop.
So later you can comment these lines when you dont require

b. Now you need to start hadoop cluster and put exportComposite and exportSimple folders(kept parallel to this readme) to hdfs 
This can be done by running the following from $HADOOP_HOME

$bin/hadoop fs -put ~/crux/testData/BseStock/exportSimple /
$bin/hadoop fs -put ~/crux/testData/BseStock/exportComposite /

export HBASE_HOME=<path to HBase folder>

$bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar import stockDataSimple /exportSimple
$bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar import stockDataComposite /exportComposite

Note:- Above process will load demo data in HBase.
If you want to load more data it can be done by following the instructions given below.

Instructions to download and populate stock data.
--------------------------------------------------

1. Fetch stock BSE data from the BSE website for a particular date range by executing downloadBseData.py.

Arguments required are 
a. filePath to a file which has the list of stockIds. Data for these stocks will be downloaded. 
b. startDate in MM/dd/yyyy format
c. endDate in MM/dd/yyyy format
d. outputPath where these files will be saved after download. This path should already exists on the filepath.

nube@nube-desktop:~/crux/testData/BseStock$ ./downloadBseData.py ./stockIdsList.txt 01/06/2011 05/06/2011 downloadedFiles/

The above command will fetch files for all stockIds listed in stockIdsList.txt kept parallel to script in BseStock folder. 
The files will be copied to 'downloadedFiles' named folder in same directory from where you run script.
Each stock's data will be saved with the name of the stockIds. 

2.Then we massage the downloaded data so that it can concatenate stockId with date(yyyyMMdd). This can be used later as the rowkey while loading.

For this make a directory named hbaseData in BseStock folder and enter the following command

nube@nube-desktop:~/crux/testData/BseStock$ ./createTableData.py downloadedFiles/ hbaseData/
where "downloadedFiles' denotes input folder and "hbaseData" denotes output folder.

Make sure hbaseData already exists.

3. We now want to save this data in HBase. Let us create a table in HBase stockDataComposite with column families price, spread and stats

On the hbase shell, this can be done as follows:
create 'stockDataSimple','data'

4. Now that the data is downloaded and prepared, we are ready to import it into HBase.

a. In First step you need to add all jars in $HBASE_HOME/lib to HADOOP_CLASSPATH for that add following line to $HADOOP_HOME/bin/hadoop file

HBASE_HOME=<path to HBase folder>
# add Hbase libs to CLASSPATH
for f in $HBASE_HOME/lib/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

Note:- These lines will add hbase dependencies to your hadoop classpath each time you run hadoop, So later you can comment these lines when you dont require

5. Load data in hbase

To put data as rowKey string 

a. Run importtsv command through hadoop to prepare data for bulk load in HBase(First set HBASE_HOME)

$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,data:openPrice,data:highPrice,data:lowPrice,data:closePrice,data:wap,data:numShares,data:numTrades,data:turnOver,data:highLow,data:closeOpen -Dimporttsv.separator=","  -Dimporttsv.bulk.output=outputHBase stockDataSimple ~/crux/testData/BseStock/hbaseData/

-Dimporttsv.columns is format of table.
-Dimporttsv.separator is to define separator in dataFile
-Dimporttsv.bulk.output is to define outputPath for this job
-stockDataSimple is tablename for which we are preparing data
-~/crux/testData/BseStock/hbaseData/ this define inputPath (is the path to the directory where files generated by createTableData.py is kept).

b.Run completebulkload command through hadoop to load data in HBase
$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar completebulkload /user/nube/outputHBase stockDataSimple

/user/nube/outputHBase this define inputPath(is output of above step (A)).
stockData is tablename where we want to insert data.
Note: In this method all data is inserted as String type.

OR

To put data as composite rowKey stockId(as dataType string), date(as dataType long) both concatenated 
 
a.Create table in HBase. On the shell,

create 'stockDataComposite','price','spread','stats'

b.Execute java class PopulateBseData found in crux/target/classes or you can compile it yourself by adding all hadoop lib jars and hbase jar.
 
nube@nube-desktop:~/crux/testData/BseStock$ export CLASSPATH=$CLASSPATH:~/hadoop-0.20.2/hadoop-0.20.2-core.jar:~/hbase-0.90.3/hbase-0.90.3.jar
nube@nube-desktop:~/crux/test/DataBseStock$ javac PopulateBseData.java
 
Once compiled, to run this program you need to add few more jars to java classpath they are commons-logging-1.1.1.jar, zookeeper-3.3.2.jar and log4j-1.2.16.jar, this code takes one argument inputpath i.e. outputPath of step 1 in this README.
Note:in this step all data inserted in table stockDataComposite as Float type except rowkey as composite key, numShares and numTrades as long type.
	
nube@nube-desktop:~/crux/testData/BseStock$ java PopulateBseData ./outputHBase
You can also add hbase jar to hadoop classpath and do
nube@nube-desktop:~/crux/target/classes$hadoop PopulateBseData ~/crux/testData/BseStock/hbaseData 

inputPath: is the path to the directory where files generated by createTableData.py is kept.
